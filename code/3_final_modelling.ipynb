{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "In this notebook, I model my dataset using both Logistic Regression w/TfidfVectorizer and Mutlinomial Naive Bayes w/CountVectorizer, based on explorative modelling done in notebook 2. I first run these models again and peek at the models' coefficients and then make changes to my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import modelling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Cleaned Data & Prepare Variables For Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "west_house = pd.read_csv('../data/west_house.csv')\n",
    "submissions = west_house[west_house['submission'] == 1]\n",
    "comments = west_house[west_house['submission'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets X variable as text column and sets y variable as subreddit column\n",
    "# Prepares train/test split\n",
    "X = west_house['text']\n",
    "y = west_house['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50069\n",
       "0    0.49931\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets a baseline accuracy score\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.7950927139913841\n",
      "Test:  0.7741444866920152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>frank</td>\n",
       "      <td>-7.215240</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>underwood</td>\n",
       "      <td>-4.442517</td>\n",
       "      <td>0.011629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>claire</td>\n",
       "      <td>-4.181368</td>\n",
       "      <td>0.015048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>card</td>\n",
       "      <td>-4.168590</td>\n",
       "      <td>0.015238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>hoc</td>\n",
       "      <td>-3.961200</td>\n",
       "      <td>0.018685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>sorkin</td>\n",
       "      <td>3.777608</td>\n",
       "      <td>0.977634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>west</td>\n",
       "      <td>4.029840</td>\n",
       "      <td>0.982533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>toby</td>\n",
       "      <td>4.058730</td>\n",
       "      <td>0.983022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>josh</td>\n",
       "      <td>4.228423</td>\n",
       "      <td>0.985634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bartlet</td>\n",
       "      <td>4.231424</td>\n",
       "      <td>0.985676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      features     coefs     probs\n",
       "65       frank -7.215240  0.000735\n",
       "218  underwood -4.442517  0.011629\n",
       "29      claire -4.181368  0.015048\n",
       "24        card -4.168590  0.015238\n",
       "82         hoc -3.961200  0.018685\n",
       "..         ...       ...       ...\n",
       "184     sorkin  3.777608  0.977634\n",
       "234       west  4.029840  0.982533\n",
       "208       toby  4.058730  0.983022\n",
       "93        josh  4.228423  0.985634\n",
       "14     bartlet  4.231424  0.985676\n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(max_df=0.25,\n",
    "                     max_features=250,\n",
    "                     min_df=3,\n",
    "                     ngram_range=(1, 1))\n",
    "X_train_lr = pd.DataFrame(tf.fit_transform(X_train).toarray(),\n",
    "                          columns=tf.get_feature_names())\n",
    "X_test_lr = pd.DataFrame(tf.transform(X_test).toarray(),\n",
    "                          columns=tf.get_feature_names())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_lr, y_train)\n",
    "print('Train: ', lr.score(X_train_lr, y_train))\n",
    "print('Test: ', lr.score(X_test_lr, y_test))\n",
    "\n",
    "# Puts coefficients in dataframe with the words they correspond to\n",
    "lr_df = pd.DataFrame({'features': X_train_lr.columns, 'coefs': lr.coef_[0]}).sort_values('coefs')\n",
    "\n",
    "# Converts coefficients into probabilities\n",
    "lr_df['probs'] = np.exp(lr_df['coefs']) / (1 + np.exp(lr_df['coefs']))\n",
    "\n",
    "# Export nb_df to use in data visualization notebook\n",
    "lr_df.to_csv('../data/logreg.csv', index=False) \n",
    "\n",
    "# Peek at results\n",
    "lr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.809327589436224\n",
      "Test:  0.8034220532319392\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>spacey</td>\n",
       "      <td>-10.531403</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>hammerschmidt</td>\n",
       "      <td>-10.531403</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>claire</td>\n",
       "      <td>-10.531403</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>frank claire</td>\n",
       "      <td>-10.531403</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>chapter</td>\n",
       "      <td>-10.531403</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>west</td>\n",
       "      <td>-4.334959</td>\n",
       "      <td>0.012933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>would</td>\n",
       "      <td>-4.287236</td>\n",
       "      <td>0.013557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>episode</td>\n",
       "      <td>-4.249136</td>\n",
       "      <td>0.014076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>show</td>\n",
       "      <td>-4.191043</td>\n",
       "      <td>0.014905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>wa</td>\n",
       "      <td>-3.496134</td>\n",
       "      <td>0.029422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          features      coefs     probs\n",
       "465         spacey -10.531403  0.000027\n",
       "204  hammerschmidt -10.531403  0.000027\n",
       "74          claire -10.531403  0.000027\n",
       "171   frank claire -10.531403  0.000027\n",
       "68         chapter -10.531403  0.000027\n",
       "..             ...        ...       ...\n",
       "558           west  -4.334959  0.012933\n",
       "582          would  -4.287236  0.013557\n",
       "129        episode  -4.249136  0.014076\n",
       "449           show  -4.191043  0.014905\n",
       "543             wa  -3.496134  0.029422\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.33,\n",
    "                         max_features=600,\n",
    "                         min_df=10,\n",
    "                         ngram_range=(1, 2))\n",
    "X_train_nb = pd.DataFrame(cv.fit_transform(X_train).toarray(),\n",
    "                          columns=cv.get_feature_names())\n",
    "X_test_nb = pd.DataFrame(cv.transform(X_test).toarray(),\n",
    "                          columns=cv.get_feature_names())\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_nb, y_train)\n",
    "print('Train: ', nb.score(X_train_nb, y_train))\n",
    "print('Test: ', nb.score(X_test_nb, y_test))\n",
    "\n",
    "# Puts coefficients in dataframe with the words they correspond to\n",
    "nb_df = pd.DataFrame({'features': X_train_nb.columns, 'coefs': nb.coef_[0]}).sort_values('coefs')\n",
    "\n",
    "# Converts coefficients into probabilities (I'm not sure if this is the same for MultiNB as it is for\n",
    "# LogReg, but the results look similar so I'm going for it. I looked into it for a while and nothing\n",
    "# quite made sense.)\n",
    "nb_df['probs'] = np.exp(nb_df['coefs'])/(1+np.exp(nb_df['coefs']))\n",
    "\n",
    "# Export nb_df to use in data visualization notebook\n",
    "nb_df.to_csv('../data/multi_nb.csv', index=False) \n",
    "\n",
    "# Peek at results\n",
    "nb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings & Data Cleaning (cont'd)\n",
    "\n",
    "\n",
    "As expected, my models picked out character names, show titles, and other key features of each show and were able to guess with pretty high accuracy whether the text came from the House of Cards or The West Wing subreddit. To make things more interesting (hopefully), I will now remove these words/names from my datasets. Though I consider this a form of preprocessing, I waited til the modelling phase so that I could see the difference in scores with and without show-specific words. I predict that my scores will drop precipitously with the removal of show-specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of words that are overtly unique to The West Wing\n",
    "west_words = ['sam', 'seaborn', 'bartlet', 'bartlett', 'allison', 'janney',\n",
    "        'cj', 'c.j.', 'cregg', 'craig', 'john', 'spencer', 'leo', 'mcgarry',\n",
    "        'bradley', 'whitford', 'josh', 'lyman', 'mandy', 'santos', \n",
    "        'martin', 'sheen', 'josiah', 'janel', 'west', 'wing',\n",
    "        'moloney', 'donna', 'moss', 'richard', 'schiff', 'toby', 'ziegler',\n",
    "        'dule', 'hill', 'charlie', 'margaret', 'rob', 'lowe', 'joshua',\n",
    "        'malina', 'will', 'bailey', 'wa', 'stockard', 'channing', 'abbey', 'aaron',\n",
    "        'sorkin', 'tommy', 'schlamme', 'misiano', 'graves', 'lawrence',\n",
    "        \"o'donnel\", 'nbc', 'tww', 'hoynes']\n",
    "\n",
    "# Creates a list of words that are overtly unique to House of Cards\n",
    "house_words = ['kevin', 'spacey', 'hammerschmidt', 'netflix', 'james', 'foley', 'andrew', 'freddy',\n",
    "              'davies', 'michael', 'dobbs', 'beau', 'willimon', 'robin', 'meechum', 'durant',\n",
    "              'wright', 'claire', 'underwood', 'michael', 'kelly', 'frank', 'dunbar', 'yates',\n",
    "              'justin', 'doescher', 'seth', 'grayson', 'zoe', 'kate', 'mara', 'remy', 'conway',\n",
    "              'russo', 'rachel', 'hoc', 'walker', 'cards', 'card', 'doug', 'house', 'tusk', 'francis',\n",
    "              'frances', 'tom', 'peter']\n",
    "\n",
    "# Combines these lists together and then combines them with the nltk library's stopwords\n",
    "west_house_words = west_words + house_words\n",
    "wrong_words = set(stopwords.words('english') + west_house_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the same function from preprocessing notebook to apply new stopwords to dataset\n",
    "# I used the function we wrote in the NLP lesson as a template\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def cleaner_strings(post):\n",
    "    post = re.sub(\"[^a-zA-Z]\", \" \", post.lower()).split()\n",
    "    post = [lemmatizer.lemmatize(i) for i in post]\n",
    "    right_words = [w for w in post if w not in wrong_words]\n",
    "    return (\" \".join(right_words))\n",
    "west_house['text'] = west_house['text'].apply(cleaner_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>trump</th>\n",
       "      <th>submission</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1478621783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1478146214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1478140783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1477743560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1477539864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1477254956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1477204646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1479245213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1478791318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1460452270</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6534</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1479902573</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6676</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1480339018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1480353312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1480364492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1476697849</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7494</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1480183592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text  subreddit  trump  submission  created_utc  word_count\n",
       "541                0      0           0   1478621783           1\n",
       "643                0      0           0   1478146214           1\n",
       "647                0      0           0   1478140783           2\n",
       "793                0      0           0   1477743560           2\n",
       "833                0      0           0   1477539864           1\n",
       "985                0      0           0   1477254956           1\n",
       "994                0      0           0   1477204646           1\n",
       "1798               0      1           0   1479245213           1\n",
       "3587               1      1           0   1478791318           1\n",
       "4419               0      0           1   1460452270           2\n",
       "6534               0      1           0   1479902573           2\n",
       "6676               0      1           0   1480339018           1\n",
       "6694               0      1           0   1480353312           1\n",
       "6723               0      1           0   1480364492           1\n",
       "7150               1      0           0   1476697849           2\n",
       "7494               1      1           0   1480183592           1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Were any text rows rendered empty by this process?\n",
    "west_house[west_house['text'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop blank rows\n",
    "west_house = west_house[west_house['text'] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling 2.0\n",
    "\n",
    "Here I use the same two models I chose as my 'best models' above, this time passing through the dataset with show-specific words removed. Though I am using the same model/vectorizer combinations, I am once again using Pipeline and GridSearch to optimize those models given the changes to the data. I expect scores to be low but coefficients to be intertesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets X variable as new version of text column and sets y variable as subreddit column\n",
    "# Prepares train/test split\n",
    "X = west_house['text']\n",
    "y = west_house['subreddit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50132\n",
       "0    0.49868\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets a baseline accuracy score\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ TfidfVectorizer\n",
    "\n",
    "Following the same pattern in notebook 2, I first optimize my hyperparamters and then recreate my model using those hyperparameters to create a dataframe of tokenized words so that I can visualize the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.6300675675675675\n",
      "Test:  0.6011428571428571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tvec__max_df': 0.25,\n",
       " 'tvec__max_features': 100,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets pipeline parameters, passes them through GridSearch, and prints out scores\n",
    "pipe_lr = Pipeline([('tvec', TfidfVectorizer()),\n",
    "                    ('lr', LogisticRegression())])\n",
    "\n",
    "# Hyperparameters were slowly tweaked to find optimal performance\n",
    "pipe_params_lr = {'tvec__max_features' : [100],\n",
    "                'tvec__min_df' : [1, 2],\n",
    "                'tvec__max_df' : [.1, .25],\n",
    "                'tvec__ngram_range' : [(1, 1), (1, 2)]}\n",
    "\n",
    "grid_lr = GridSearchCV(pipe_lr, \n",
    "                  pipe_params_lr,\n",
    "                  cv=5)\n",
    "grid_lr.fit(X_train, y_train)\n",
    "mod_lr = grid_lr.best_estimator_\n",
    "print('Train: ', mod_lr.score(X_train, y_train))\n",
    "print('Test: ', mod_lr.score(X_test, y_test))\n",
    "grid_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.628566066066066\n",
      "Test:  0.603047619047619\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>spoiler</td>\n",
       "      <td>-3.310273</td>\n",
       "      <td>0.035220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>season</td>\n",
       "      <td>-2.557993</td>\n",
       "      <td>0.071891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>www</td>\n",
       "      <td>-1.157475</td>\n",
       "      <td>0.239126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lot</td>\n",
       "      <td>-0.733047</td>\n",
       "      <td>0.324526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>shot</td>\n",
       "      <td>-0.570617</td>\n",
       "      <td>0.361094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>watch</td>\n",
       "      <td>1.162855</td>\n",
       "      <td>0.761851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>need</td>\n",
       "      <td>1.321812</td>\n",
       "      <td>0.789483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>line</td>\n",
       "      <td>1.533307</td>\n",
       "      <td>0.822490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>episode</td>\n",
       "      <td>1.718025</td>\n",
       "      <td>0.847874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>white</td>\n",
       "      <td>1.788543</td>\n",
       "      <td>0.856749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   features     coefs     probs\n",
       "75  spoiler -3.310273  0.035220\n",
       "67   season -2.557993  0.071891\n",
       "98      www -1.157475  0.239126\n",
       "42      lot -0.733047  0.324526\n",
       "71     shot -0.570617  0.361094\n",
       "..      ...       ...       ...\n",
       "90    watch  1.162855  0.761851\n",
       "49     need  1.321812  0.789483\n",
       "40     line  1.533307  0.822490\n",
       "18  episode  1.718025  0.847874\n",
       "95    white  1.788543  0.856749\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(max_df=0.25,\n",
    "                     max_features=100,\n",
    "                     min_df=1,\n",
    "                     ngram_range=(1, 2))\n",
    "X_train_lr = pd.DataFrame(tf.fit_transform(X_train).toarray(),\n",
    "                          columns=tf.get_feature_names())\n",
    "X_test_lr = pd.DataFrame(tf.transform(X_test).toarray(),\n",
    "                          columns=tf.get_feature_names())\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_lr, y_train)\n",
    "print('Train: ', lr.score(X_train_lr, y_train))\n",
    "print('Test: ', lr.score(X_test_lr, y_test))\n",
    "\n",
    "# Puts coefficients in dataframe with the words they correspond to\n",
    "lr_df = pd.DataFrame({'features': X_train_lr.columns, 'coefs': lr.coef_[0]}).sort_values('coefs')\n",
    "\n",
    "# Converts coefficients into probabilities\n",
    "lr_df['probs'] = np.exp(lr_df['coefs']) / (1 + np.exp(lr_df['coefs']))\n",
    "\n",
    "# Export nb_df to use in data visualization notebook\n",
    "lr_df.to_csv('../data/logreg2.csv', index=False) \n",
    "\n",
    "# Peek at results\n",
    "lr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes w/ CountVectorizer\n",
    "\n",
    "Following the same pattern above, I first optimize my hyperparamters and then recreate my model using those hyperparameters to create a dataframe of tokenized words so that I can visualize the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.6931306306306306\n",
      "Test:  0.6636190476190477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.25,\n",
       " 'cvec__max_features': 400,\n",
       " 'cvec__min_df': 10,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets pipeline parameters, passes them through GridSearch, and prints out scores\n",
    "pipe_nb = Pipeline([('cvec', CountVectorizer()),\n",
    "                    ('nb', MultinomialNB())])\n",
    "\n",
    "# Hyperparameters were slowly tweaked to find optimal performance\n",
    "pipe_params_nb = {'cvec__max_features' : [400],\n",
    "                'cvec__min_df' : [10, 20],\n",
    "                'cvec__max_df' : [.25, .4],\n",
    "                'cvec__ngram_range' : [(1, 1), (1, 2)]}\n",
    "\n",
    "grid_nb = GridSearchCV(pipe_nb, \n",
    "                  pipe_params_nb,\n",
    "                  cv=5)\n",
    "grid_nb.fit(X_train, y_train)\n",
    "mod_nb = grid_nb.best_estimator_\n",
    "print('Train: ', mod_nb.score(X_train, y_train))\n",
    "print('Test: ', mod_nb.score(X_test, y_test))\n",
    "grid_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.6950075075075075\n",
      "Test:  0.668952380952381\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>chapter</td>\n",
       "      <td>-10.244770</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>trailer</td>\n",
       "      <td>-8.858475</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>electoral</td>\n",
       "      <td>-7.942185</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>war</td>\n",
       "      <td>-7.942185</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>theory</td>\n",
       "      <td>-7.759863</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>like</td>\n",
       "      <td>-4.148945</td>\n",
       "      <td>0.015781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>president</td>\n",
       "      <td>-4.094167</td>\n",
       "      <td>0.016670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>would</td>\n",
       "      <td>-4.062685</td>\n",
       "      <td>0.017203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>show</td>\n",
       "      <td>-3.973781</td>\n",
       "      <td>0.018802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>episode</td>\n",
       "      <td>-3.918620</td>\n",
       "      <td>0.019868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      features      coefs     probs\n",
       "44     chapter -10.244770  0.000036\n",
       "347    trailer  -8.858475  0.000142\n",
       "77   electoral  -7.942185  0.000355\n",
       "369        war  -7.942185  0.000355\n",
       "332     theory  -7.759863  0.000427\n",
       "..         ...        ...       ...\n",
       "169       like  -4.148945  0.015781\n",
       "251  president  -4.094167  0.016670\n",
       "390      would  -4.062685  0.017203\n",
       "299       show  -3.973781  0.018802\n",
       "82     episode  -3.918620  0.019868\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.25,\n",
    "                         max_features=400,\n",
    "                         min_df=10,\n",
    "                         ngram_range=(1, 1))\n",
    "X_train_nb = pd.DataFrame(cv.fit_transform(X_train).toarray(),\n",
    "                          columns=cv.get_feature_names())\n",
    "X_test_nb = pd.DataFrame(cv.transform(X_test).toarray(),\n",
    "                          columns=cv.get_feature_names())\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_nb, y_train)\n",
    "print('Train: ', nb.score(X_train_nb, y_train))\n",
    "print('Test: ', nb.score(X_test_nb, y_test))\n",
    "\n",
    "# Puts coefficients in dataframe with the words they correspond to\n",
    "nb_df = pd.DataFrame({'features': X_train_nb.columns, 'coefs': nb.coef_[0]}).sort_values('coefs')\n",
    "\n",
    "# Converts coefficients into probabilities (I'm not sure if this is the same for MultiNB as it is for\n",
    "# LogReg, but the results look similar so I'm going for it. I looked into it for a while and nothing\n",
    "# quite made sense.)\n",
    "nb_df['probs'] = np.exp(nb_df['coefs'])\n",
    "\n",
    "# Export nb_df to use in data visualization notebook\n",
    "nb_df.to_csv('../data/multi_nb2.csv', index=False) \n",
    "\n",
    "# Peek at results\n",
    "nb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready For Trump Side Project And Data Visualization\n",
    "\n",
    "Now that my data is free of show-specific words and my main modelling is done, I am ready to export my dataframe as a CSV file to use for data visualization and my further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "west_house.to_csv('../data/west_house_2.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
